{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4983743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nvidia and Foxconn have announced a joint plan to build a large-scale AI supercomputer in Taiwan, which will be operated by Foxconn's subsidiary, Big Innovation Company. Revealed during the Computex 2025 trade show, the project is backed by Taiwan's National Science and Technology Council and is being described as a national AI infrastructure initiative.\n",
      "\n",
      "The supercomputer will feature 10,000 of Nvidia's latest Blackwell GPUs and will incorporate the company's Blackwell Ultra systems, including the GB300 NVL72 rack-scale platform. It is expected to become one of the most powerful AI computing systems in Asia, offering access to startups, researchers, and enterprises across various sectors. The infrastructure will be integrated with Nvidia DGX Cloud Lepton, enabling flexible GPU use for a wide range of users.\n",
      "\n",
      "Foxconn Chairman Young Liu said the facility will be built in phases and will eventually reach 100 megawatts of power, depending on electricity availability. \"We know that power is a very critical resource in Taiwan. I don't want to use the word 'shortage'. So it will take a few steps to reach 100 megawatts. We'll start with 20 megawattsâ€¦ then add another 40,\" he said. Initial development is planned in the city of Kaohsiung, with additional locations under consideration.\n",
      "\n",
      "The AI computing centre is part of Taiwan's broader aim to strengthen its AI capabilities and support domestic innovation. Government officials have indicated that the facility will be central to efforts in AI education, healthcare, urban planning, and industry-led research.\n",
      "\n",
      "Nvidia CEO Jensen Huang, speaking at the event, described the project as an asset for Taiwan's wider tech community. \"We're going to build an AI factory right for you (Foxconn) to use, for me to use, and for Taiwan's entire ecosystem to use,\" he said, noting that Nvidia currently has around 350 partners in the country.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"nvidia.txt\")\n",
    "data = loader.load()\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8f1fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51039819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unstructured in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: libmagic in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (1.0)\n",
      "Requirement already satisfied: python-magic in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (0.4.27)\n",
      "Requirement already satisfied: python-magic-bin in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (0.4.14)\n",
      "Requirement already satisfied: chardet in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured) (5.2.0)\n",
      "Requirement already satisfied: filetype in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured) (5.4.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured) (3.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured) (4.13.4)\n",
      "Requirement already satisfied: emoji in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured) (2.14.1)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured) (0.6.7)\n",
      "Requirement already satisfied: python-iso639 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured) (2025.2.18)\n",
      "Requirement already satisfied: langdetect in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured) (1.0.9)\n",
      "Requirement already satisfied: numpy in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured) (2.2.6)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured) (3.13.0)\n",
      "Requirement already satisfied: backoff in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured) (4.13.2)\n",
      "Requirement already satisfied: unstructured-client in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured) (0.35.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured) (1.17.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured) (7.0.0)\n",
      "Requirement already satisfied: python-oxmsg in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured) (0.0.2)\n",
      "Requirement already satisfied: html5lib in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from beautifulsoup4->unstructured) (2.7)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from dataclasses-json->unstructured) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (24.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from html5lib->unstructured) (1.17.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from html5lib->unstructured) (0.5.1)\n",
      "Requirement already satisfied: click in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from nltk->unstructured) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from nltk->unstructured) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from nltk->unstructured) (2024.11.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from click->nltk->unstructured) (0.4.6)\n",
      "Requirement already satisfied: olefile in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from python-oxmsg->unstructured) (0.47)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from requests->unstructured) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from requests->unstructured) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from requests->unstructured) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from requests->unstructured) (2025.4.26)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured-client->unstructured) (24.1.0)\n",
      "Requirement already satisfied: cryptography>=3.1 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured-client->unstructured) (45.0.3)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured-client->unstructured) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Requirement already satisfied: pydantic>=2.11.2 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured-client->unstructured) (2.11.5)\n",
      "Requirement already satisfied: pypdf>=4.0 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured-client->unstructured) (5.5.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from cffi>=1.14->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
      "Requirement already satisfied: anyio in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.4.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install unstructured libmagic python-magic python-magic-bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39758d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zacks\n",
      "\n",
      "Zacks\n",
      "\n",
      "NVIDIA vs. Super Micro: Which AI Hardware Stock Should You Bet On?\n",
      "\n",
      "4 min read\n",
      "\n",
      "\n",
      "\n",
      "NVDA\n",
      "\n",
      "\n",
      "\n",
      "SMCI\n",
      "\n",
      "The artificial intelligence (AI) revolution has ignited a race among hardware suppliers, and two of the most prominent names riding this wave are NVIDIA Corporation NVDA and Super Micro Computer, Inc. SMCI. NVIDIA dominates the AI graphics processing unit (GPU) market, while Super Micro rides the same wave by designing ultra-efficient, NVIDIA’s chip-optimized servers for hyperscalers and enterprises.\n",
      "\n",
      "Both stocks have delivered exceptional returns, but which is the better bet now? Let’s break down their fundamentals, valuation, growth outlook and looming risks to find out which one is a better investment option today.\n",
      "\n",
      "NVDA: The AI Chip Giant With Dominance\n",
      "\n",
      "NVIDIA is an undisputed leader in AI chips, data centers, gaming and autonomous vehicles. Its products are at the center of the ongoing AI revolution, driving demand from hyperscalers, enterprises and cutting-edge startups alike. The data center end-market continues to be a powerhouse for NVIDIA. Revenues from this end-market surged 93% year over year to $35.58 billion in the fourth quarter of fiscal 2025.\n",
      "\n",
      "NVIDIA’s latest earnings call underscored the company’s continued AI dominance. CEO Jensen Huang highlighted the increasing demand for next-generation AI models that require unprecedented computational power. The company’s Blackwell architecture, capable of delivering up to 25 times the token throughput of its predecessor, is expected to drive the next wave of AI adoption.\n",
      "\n",
      "Further bolstering its leadership, NVIDIA is set to launch its Blackwell Ultra and Vera Rubin platforms, which could solidify its position as the go-to AI infrastructure provider. With governments, corporations and cloud providers ramping up AI investments, NVIDIA remains the key beneficiary of this seismic shift in computing.\n",
      "\n",
      "However, the recent restrictions imposed by the Trump administration on exporting H20 chips to China are likely to hurt NVIDIA’s overall financial growth in the near term. The company’s CEO, Jensen Huang, recently stated that export restrictions on H20 chips have cost the company $15 billion in sales and expects approximately $5.5 billion in charges due to the ban in the first quarter of fiscal 2026.\n",
      "\n",
      "Despite the headwinds, NVIDIA is anticipated to remain on a high-growth trajectory. The company expects revenues of $43 billion (+/-2%) in the first quarter of fiscal 2026, indicating a year-over-year growth of more than 65%.\n",
      "\n",
      "SMCI: A High-Risk, High-Reward AI Infrastructure Play\n",
      "\n",
      "Super Micro Computer’s growth is driven by the need for AI workloads. As a growing number of data centers are proliferating and existing ones are expanding their capacity, the need for SMCI’s high-performance and energy-efficient servers is rising.\n",
      "\n",
      "Super Micro Computer’s liquid-cooled and modular servers are a hit among cloud service providers, government customers and enterprises as these servers possess the capacity to handle AI at scale. SMCI has further strengthened its AI expertise by collaborating with NVIDIA and integrating its Blackwell GPUs for high compute power.\n",
      "\n",
      "Despite the massive potential of Super Micro Computer’s server offerings, the company is facing some near-term challenges, including delayed purchasing decisions from customers as they are evaluating the adoption of next-generation AI platforms. SMCI is also facing margin contraction due to the growing price competition and price adjustments as companies are second-guessing their shift from older to newer platforms like Blackwell.\n",
      "\n",
      "In the last reported results for the third quarter of fiscal 2025, Super Micro Computer also incurred a one-time inventory write-down on older-generation GPUs and related components, further affecting its margins. Based on all the above factors, SMCI revised the revenue guidance for fiscal 2025 from the $23.5-$25.0 billion range to a range of $21.8 billion-$22.6 billion.\n",
      "\n",
      "EPS Estimate Trends: NVDA Stays Less Volatile Than SMCI\n",
      "\n",
      "The Zacks Consensus Estimate for Super Micro Computer’s fiscal 2025 EPS indicates a year-over-year decline of 6%, while that for fiscal 2026 signifies growth of 36%. The earnings estimate revision trend for the two fiscals has remained highly volatile over the last 60 days.\n",
      "\n",
      "SMCI Earnings Estimate Revision Trend\n",
      "\n",
      "The consensus mark for NVIDIA’s fiscal 2026 and fiscal 2027 EPS indicates year-over-year growth of 43% and 27%, respectively. Also, the earnings estimate revision trend for the stock has remained more stable compared with a highly volatile trend for SMCI. This stability is a testament to NVDA’s predictable performance in a volatile sector.\n",
      "\n",
      "NVDA Earnings Estimate Revision Trend\n",
      "\n",
      "NVDA vs. SMCI: Price Performance & Valuation Check\n",
      "\n",
      "Both companies have seen share price fluctuations over the past year amid geopolitical tensions and macroeconomic uncertainty. While NVDA shares have soared 42.8% over the past year, SMCI dropped 50.6%.\n",
      "\n",
      "Valuation-wise, both NVIDIA and Super Micro Computer are trading at earnings multiples below their respective industry averages. However, NVDA’s current price-to-earnings (P/E) multiple of 29.25X is significantly lower than the one-year median of 37.28X. On the other hand, SMCI’s P/E multiple of 16.41X is slightly higher than the one-year median of 16.10X.\n",
      "\n",
      "While SMCI appears cheaper on a P/E basis, it comes with significantly more volatility and execution risk.\n",
      "\n",
      "Conclusion: NVIDIA is the Better AI Hardware Bet Right Now\n",
      "\n",
      "While both NVIDIA and Super Micro Computer are important players in the AI revolution, NVIDIA clearly stands out as the stronger investment today.\n",
      "\n",
      "NVIDIA’s dominant market position, product innovation and earnings consistency make it the AI infrastructure stock of choice. Despite geopolitical challenges, the company is still expected to deliver blockbuster growth in fiscal 2026 and beyond, driven by its next-generation Blackwell architecture and robust customer demand.\n",
      "\n",
      "Meanwhile, Super Micro Computer is more of a high-risk, high-reward satellite play. It remains a compelling company with niche strengths, but its exposure to product cycles, customer hesitation and margin pressure makes its near-term outlook more uncertain.\n",
      "\n",
      "Furthermore, NVDA carries a Zacks Rank #3 (Hold) at present, making the stock a stronger pick compared with SMCI, which has a Zacks Rank #4 (Sell).\n",
      "\n",
      "You can see the complete list of today’s Zacks #1 Rank (Strong Buy) stocks here.\n",
      "\n",
      "This article originally published on Zacks Investment Research (zacks.com).\n",
      "\n",
      "Zacks Investment Research\n",
      "\n",
      "Zacks\n",
      "\n",
      "Read more from Zacks\n",
      "\n",
      "Compare charts\n",
      "\n",
      "Analyze on Supercharts\n",
      "\n",
      "Latest news\n",
      "\n",
      "Live\n",
      "\n",
      "More news from Zacks\n",
      "Why Did Nvidia Stock Drop afte...\n",
      "\n",
      "Home\n",
      "\n",
      "/News\n",
      "\n",
      "/Why Did Nvidia Stock Drop after Computex Keynote?\n",
      "\n",
      "Why Did Nvidia Stock Drop after Computex Keynote?\n",
      "\n",
      "Nvidia stock is low today but is higher than it was a few months ago, making steady progress toward its pre-tariff highs.\n",
      "\n",
      "Timothy St. John•Tuesday, May 20, 2025•2 min read\n",
      "\n",
      "Nvidia stock has recovered recently but is down today.\n",
      "\n",
      "Quick overview\n",
      "\n",
      "Nvidia's stock dropped 1.96% following a lackluster keynote at Computex 2025, failing to deliver any surprising announcements.\n",
      "\n",
      "The company announced plans to target the AI inference market with RTX Pro servers and introduced NVlink Fusion for custom AI infrastructure.\n",
      "\n",
      "Despite the recent stock dip, Nvidia has shown resilience and long-term potential, with expectations of growth as the AI market expands.\n",
      "\n",
      "Investors are advised to consider buying during dips for short-term profits, as Nvidia is likely to recover lost ground soon.\n",
      "\n",
      "After Nvidia (NVDA) presented its keynote at Computex 2025, the company’s stock dropped 1.96%. What should have been a great success for the leading AI tech company turned out to be a miss.\n",
      "\n",
      "Nvidia failed to impress with anything surprising at the Computex conference. The AI chip manufacturer announced that it would be aiming to reach the AI inference market with RTX Pro servers. They also announced that they would be creating NVlink Fusion, which expands the company’s reach beyond Nvidia CPUs.\n",
      "\n",
      "The Fusion is a silicon that would allow for partially custom built AI infrastructure. That expands the company’s product capabilities and lets customers do more with their Nvidia products.\n",
      "\n",
      "Nvidia Stock- Buy, Sell or Wait?\n",
      "\n",
      "Over the last 24 hours, Nvidia’s stock has fallen nearly 2%. On the other hand, in the last five days of trading, the stock has remained active but stable, fluctuating between $132 and $134. The three month view is more telling. In that period, the stock was high around $126 at the start of the period, dropped to a low of $94, and then climbed back up to its present $134 price.\n",
      "\n",
      "What we are seeing is that Nvidia is resilient. This is a stock that has performed well for a few years now and will likely continue to do so. If you have NVDA stock, it does not make sense to sell it right now, since it could easily go much higher, unless you bought it when it was very low.\n",
      "\n",
      "Nvidia stock has tremendous long-term potential, and as long as the AI market is doing well, Nvidia is likely to do well. The company has proven that they can weather storms for monopoly allegations, fierce competition, and even severe tariffs, and they can eventually come out on top. We expect this stock to do very well during the 90-day pause on tariffs, climbing much higher than its current price.\n",
      "\n",
      "Even though the Computex conference did not do much for Nvidia’s stock price, the rapidly growing AI market should help keep it moving along. For those wanting short-term profits, we suggest buying during the next dip, because Nvidia is sure to gain back lost ground shortly afterwards.\n",
      "\n",
      "Check out our free forex signals\n",
      "\n",
      "Follow the top economic events on FX Leaders economic calendar\n",
      "\n",
      "Trade better, discover more Forex Trading Strategies\n",
      "\n",
      "Open a FREE Trading Account\n",
      "\n",
      "ABOUT THE AUTHOR See More\n",
      "\n",
      "\n",
      "\n",
      "Timothy St. John\n",
      "\n",
      "Financial Writer - European & US Desks\n",
      "\n",
      "Timothy St John is a seasoned financial analyst and writer, catering to the dynamic landscapes of the US and European markets. Boasting over a decade of extensive freelance writing experience, he has made significant contributions to reputable platforms such as Yahoo!Finance, business.com: Expert Business Advice, Tips, and Resources - Business.com, and numerous others. Timothy's expertise lies in in-depth research and comprehensive coverage of stock and cryptocurrency movements, coupled with a keen understanding of the economic factors influencing currency dynamics. Timothy majored in English at East Tennessee State University, and you can find him on LinkedIn.\n",
      "\n",
      "Related Articles\n",
      "\n",
      "img\n",
      "\n",
      "Stocks Rebound Strongly to End Losing Streak\n",
      "\n",
      "3 hours ago\n",
      "\n",
      "Save\n",
      "\n",
      "img\n",
      "\n",
      "Nvidia’s Stock Jumps ahead of Q1 Earnings Report\n",
      "\n",
      "1 day ago\n",
      "\n",
      "Save\n",
      "\n",
      "img\n",
      "\n",
      "Trump Promises Delay on New Tariffs and Stock Market Reacts\n",
      "\n",
      "1 day ago\n",
      "\n",
      "Save\n",
      "\n",
      "Sidebar rates\n",
      "\n",
      "trend\n",
      "\n",
      "EUR/USD0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "USD/JPY0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "DOW0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "GBP/USD0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "DAX0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "GOLD0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "EUR/USD0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "USD/JPY0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "AUD/USD0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "GBP/USD0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "EUR/GBP0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "USD/CHF0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "BTC0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "BNB0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "DOT0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "ETH0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "ADA0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "XRP0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "USOil0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "SILVER0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "PLATINUM0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "UKOil0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "GOLD0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "PALLADIUM0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "DAX0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "S&P5000.0000\n",
      "\n",
      "trend\n",
      "\n",
      "DOW0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "CAC0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "FTSE0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "NIKKEI2250.0000\n",
      "\n",
      "HFM\n",
      "\n",
      "Related Posts\n",
      "\n",
      "img\n",
      "\n",
      "Stocks Rebound Strongly to End Losing Streak\n",
      "\n",
      "3 hours ago\n",
      "\n",
      "\n",
      "\n",
      "Save\n",
      "\n",
      "img\n",
      "\n",
      "Nvidia’s Stock Jumps ahead of Q1 Earnings Report\n",
      "\n",
      "1 day ago\n",
      "\n",
      "\n",
      "\n",
      "Save\n",
      "\n",
      "img\n",
      "\n",
      "Trump Promises Delay on New Tariffs and Stock Market Reacts\n",
      "\n",
      "1 day ago\n",
      "\n",
      "\n",
      "\n",
      "Save\n",
      "\n",
      "Doo Prime\n",
      "\n",
      "XM\n",
      "\n",
      "Best Forex Brokers\n",
      "\n",
      "\n",
      "\n",
      "subscribe-icon\n",
      "\n",
      "Join 350 000+ traders receiving Free Trading Signals\n",
      "\n",
      "About FX Leaders\n",
      "\n",
      "FX Leaders is an information station for forex, com – modities, indices and cryptocurrency traders. Providing you with the best strategies and trading opportunities whilst equipping you with the tools you need to be successful. Get free trading signals , daily market insights, tips, the best educational resources, social trading and much more...\n",
      "\n",
      "Frequently Asked Questions\n",
      "\n",
      "Is FX Leaders free to use?\n",
      "\n",
      "Absolutely! All services are free, so you to take advantage of the opportunities that Forex trading offers.\n",
      "\n",
      "See our FAQ's\n",
      "\n",
      "Become a better trader\n",
      "\n",
      "FX Leaders Learn Center\n",
      "\n",
      "FXL's FREE Forex Course\n",
      "\n",
      "How to choose a Forex broker\n",
      "\n",
      "Top Recommended Forex Brokers\n",
      "\n",
      "Forex Trading Strategies\n",
      "\n",
      "How do I start?\n",
      "\n",
      "Open a trading account with one of our recommended brokers and start trading by following our forex signals and trade strategies!\n",
      "\n",
      "Home\n",
      "\n",
      "News\n",
      "\n",
      "Forex Signals\n",
      "\n",
      "How to use Forex Signals\n",
      "\n",
      "Premium Membership\n",
      "\n",
      "Affiliates\n",
      "\n",
      "Learn Center\n",
      "\n",
      "Forex Trading Course\n",
      "\n",
      "Forex Trading Strategies\n",
      "\n",
      "FAQ\n",
      "\n",
      "Trading Glossary\n",
      "\n",
      "Learn Crypto\n",
      "\n",
      "Crypto Guides\n",
      "\n",
      "Crypto Strategies\n",
      "\n",
      "Rebates\n",
      "\n",
      "Economic Calendar\n",
      "\n",
      "Live Rates\n",
      "\n",
      "Converter\n",
      "\n",
      "Charts\n",
      "\n",
      "Price Forecasts\n",
      "\n",
      "Signals Report\n",
      "\n",
      "Crypto Signals\n",
      "\n",
      "Stock Signals\n",
      "\n",
      "Forex Brokers\n",
      "\n",
      "Forex Brokers Guide\n",
      "\n",
      "How to choose a Broker\n",
      "\n",
      "Forex Trading Platforms\n",
      "\n",
      "Sources and Partners\n",
      "\n",
      "Watchlist Tutorial\n",
      "\n",
      "Signals Tutorial\n",
      "\n",
      "Testimonials\n",
      "\n",
      "Advertise\n",
      "\n",
      "About Us\n",
      "\n",
      "Contact us\n",
      "\n",
      "Mobile app\n",
      "\n",
      "Sitemap\n",
      "\n",
      "Best Brokers by Type\n",
      "\n",
      "Best Forex Brokers\n",
      "\n",
      "Best Forex Trading Apps\n",
      "\n",
      "Best Forex No Deposit Bonus\n",
      "\n",
      "Best CFD Brokers\n",
      "\n",
      "Best Nasdaq Forex Brokers\n",
      "\n",
      "Best Copy Trading Brokers\n",
      "\n",
      "Best Low Spread Forex Brokers\n",
      "\n",
      "Best Swap Free Forex Brokers\n",
      "\n",
      "Best ECN Forex Brokers\n",
      "\n",
      "Best STP Forex Brokers\n",
      "\n",
      "Best Cent Account Forex Brokers\n",
      "\n",
      "Best Metatrader Brokers\n",
      "\n",
      "Trusted Broker Reviews\n",
      "\n",
      "Avatrade Review\n",
      "\n",
      "Plus500 Review\n",
      "\n",
      "Exness Review\n",
      "\n",
      "HFM Review\n",
      "\n",
      "Tickmill Review\n",
      "\n",
      "IC Markets Review\n",
      "\n",
      "XM Review\n",
      "\n",
      "JustMarkets Review\n",
      "\n",
      "Roboforex Review\n",
      "\n",
      "Pepperstone Review\n",
      "\n",
      "FXPro Review\n",
      "\n",
      "Eightcap Review\n",
      "\n",
      "Forex Brokers by Country\n",
      "\n",
      "Best Forex Brokers in UK\n",
      "\n",
      "Best Forex Brokers in South Africa\n",
      "\n",
      "Best Forex Brokers in Malaysia\n",
      "\n",
      "Best Forex Brokers in Singapore\n",
      "\n",
      "Best Forex Brokers in Indonesia\n",
      "\n",
      "Best Forex Brokers in India\n",
      "\n",
      "Best Forex Brokers in Philippines\n",
      "\n",
      "Best Forex Brokers in United States\n",
      "\n",
      "Best Forex Brokers in Canada\n",
      "\n",
      "Best Forex Brokers in Nigeria\n",
      "\n",
      "Best Forex Brokers in Kenya\n",
      "\n",
      "Best Forex Brokers in UAE\n",
      "\n",
      "Add FX Leaders to your Google News feed.\n",
      "\n",
      "\n",
      "\n",
      "Risk Warning\n",
      "\n",
      "Trading forex, cryptocurrencies, indices, and commodities are potentially high risk and may not be suitable for all investors. The high level of leverage can work both for and against traders. Before any investment in forex, cryptocurrencies, indices, and commodities you need to carefully consider your targets, previous experience, and risk level. Trading may result in the loss of your money, therefore, you should not invest capital that you cannot afford to lose.\n",
      "\n",
      "Address\n",
      "\n",
      "Arabella 3 - Mudon - Villa 444 - 22A St - Dubai - United Arab Emirates\n",
      "\n",
      "CREATE ACCOUNT\n",
      "\n",
      "GET MARKET OPPORTUNITIES BEFORE EVERYONE ELSE\n",
      "\n",
      "Daily updates directly to your email\n",
      "\n",
      "Customize your watchlist to suit your needs\n",
      "\n",
      "Register now\n",
      "\n",
      "mobile\n",
      "\n",
      "FX LEADERS ON MOBILE\n",
      "\n",
      "phones-img\n",
      "\n",
      "app-store\n",
      "\n",
      "play-store\n",
      "\n",
      "FXL Logo\n",
      "\n",
      "Copyright 2012-2025 by FX Leaders Terms Of Use, Privacy Policy, Disclaimer, Sitemap\n",
      "\n",
      "trend\n",
      "\n",
      "BTC 0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "ETH 0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "EUR/USD 0.0000\n",
      "\n",
      "trend\n",
      "\n",
      "DOW 0.0000\n",
      "\n",
      "Alerts Rates Calendar Signals\n",
      "\n",
      "Copied to clipboard!\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "URLLoader = UnstructuredURLLoader( urls=   [\n",
    "    \"https://www.tradingview.com/news/zacks:16898bb2a094b:0-nvidia-vs-super-micro-which-ai-hardware-stock-should-you-bet-on/\",\n",
    "    \"https://www.fxleaders.com/news/2025/05/20/why-did-nvidia-stock-drop-after-computex-keynote/\"\n",
    "])\n",
    "\n",
    "data = URLLoader.load()\n",
    "print(data[0].page_content)\n",
    "print(data[1].page_content)                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cf648de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5664f188",
   "metadata": {},
   "source": [
    "# Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77505715",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Interstellar is a 2014 epic science fiction film directed by Christopher Nolan, who co-wrote the screenplay with his brother Jonathan. It features an ensemble cast led by Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, and Michael Caine. Set in a dystopian future where Earth is suffering from catastrophic blight and famine, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for mankind.\n",
    "\n",
    "The screenplay had its origins in a script Jonathan developed in 2007 and was originally set to be directed by Steven Spielberg. Theoretical physicist Kip Thorne was an executive producer and scientific consultant on the film, and wrote the tie-in book The Science of Interstellar. It was Lynda Obst's final film as producer before her death. Cinematographer Hoyte van Hoytema shot it on 35 mm film in the Panavision anamorphic format and IMAX 70 mm. Filming began in late 2013 and took place in Alberta, Klaustur, and Los Angeles. Interstellar uses extensive practical and miniature effects, and the company DNEG created additional digital effects.\n",
    "\n",
    "Interstellar premiered at the TCL Chinese Theatre on October 26, 2014, and was released in theaters in the United States on November 5, and in the United Kingdom on November 7. In the United States, it was first released on film stock, expanding to venues using digital projectors. The film received generally positive reviews from critics and was a commercial success, grossing $681 million worldwide during its initial theatrical run, and $758.6 million worldwide with subsequent releases, making it the tenth-highest-grossing film of 2014. In addition to being named by the American Film Institute one of the top-ten films of 2014, Intestellar received various accolades, including five Academy Awards nominations at the 87th Academy Awards, and won Best Visual Effects.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9ca5f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 476, which is longer than the specified 200\n",
      "Created a chunk of size 649, which is longer than the specified 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "chunks = splitter.split_text(text)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3625080e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "rec_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=['\\n\\n', '\\n', ''],\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0,)\n",
    "rec_chunks = rec_splitter.split_text(text)\n",
    "len(rec_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9fffeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from faiss-cpu) (2.2.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.11.0-cp312-cp312-win_amd64.whl (15.0 MB)\n",
      "   ---------------------------------------- 0.0/15.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.3/15.0 MB 22.3 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 1.3/15.0 MB 22.3 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 1.6/15.0 MB 2.3 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 4.5/15.0 MB 5.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 8.7/15.0 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.1/15.0 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.0/15.0 MB 11.2 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae79a90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Using cached huggingface_hub-0.32.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aumpa\\onedrive\\documents\\github\\langchain\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Using cached transformers-4.52.3-py3-none-any.whl (10.5 MB)\n",
      "Using cached huggingface_hub-0.32.2-py3-none-any.whl (509 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-win_amd64.whl (11.1 MB)\n",
      "Installing collected packages: scikit-learn, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   ---------------------------------------- 0/5 [scikit-learn]\n",
      "   -------- ------------------------------- 1/5 [huggingface-hub]\n",
      "   -------- ------------------------------- 1/5 [huggingface-hub]\n",
      "   -------- ------------------------------- 1/5 [huggingface-hub]\n",
      "   ---------------- ----------------------- 2/5 [tokenizers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   ---------------------------------------- 5/5 [sentence-transformers]\n",
      "\n",
      "Successfully installed huggingface-hub-0.32.2 scikit-learn-1.6.1 sentence-transformers-4.1.0 tokenizers-0.21.1 transformers-4.52.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7caff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
